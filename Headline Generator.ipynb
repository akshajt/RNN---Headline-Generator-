{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare sequence data to use in a recurrent neural network (RNN)\n",
    "* Build and train a model to perform word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "nyt_dir = 'data/nyt_dataset/articles/'\n",
    "\n",
    "all_headlines = []\n",
    "for filename in os.listdir(nyt_dir):\n",
    "    if 'Articles' in filename:\n",
    "        # Read in all the data from the CSV file\n",
    "        headlines_df = pd.read_csv(nyt_dir + filename)\n",
    "        # Add all of the headlines to our list\n",
    "        all_headlines.extend(list(headlines_df.headline.values))\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all headlines with the value of \"Unknown\"\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize the words in headlines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total words: ', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a subset of the word_index dictionary created by Tokenizer\n",
    "subset_dict = {key: value for key, value in tokenizer.word_index.items() \\\n",
    "               if key in ['a','man','a','plan','a','canal','panama']}\n",
    "print(subset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see how the tokenizer actually saves the words\n",
    "tokenizer.texts_to_sequences(['a','man','a','plan','a','canal','panama'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to sequence of tokens \n",
    "input_sequences = []\n",
    "for line in all_headlines:\n",
    "    # Convert our headline into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    # Create a series of sequences for each headline\n",
    "    for i in range(1, len(token_list)):\n",
    "        partial_sequence = token_list[:i+1]\n",
    "        input_sequences.append(partial_sequence)\n",
    "\n",
    "print(tokenizer.sequences_to_texts(input_sequences[:5]))\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make all the sequences the same length\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Determine max sequence length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad all sequences with zeros at the beginning to make them all max length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Predictors and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors are every word except the last\n",
    "predictors = input_sequences[:,:-1]\n",
    "# Labels are the last word\n",
    "labels = input_sequences[:,-1]\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like our earlier sections, these targets are categorical. We are predicting one word out of our possible total vocabulary. Instead of the network predicting scalar numbers, we will have it predict binary categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "\n",
    "labels = utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Input is max sequence length - 1, as we've removed the last word for the label\n",
    "input_len = max_sequence_len - 1 \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add input embedding layer\n",
    "# Layer takes the tokenized sequence and learns an embedding for all the words in the dataset.\n",
    "model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "# Add LSTM layer with 100 units\n",
    "# Type of RNN which, unlike feed forward neural networks, has loops in them\n",
    "# Normal RNNs suffer from information recency bias so LSTMs provide a way to retain longer term information.\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(predictors, labels, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    prediction = model.predict_classes(token_list, verbose=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_next_token(\"today in new york\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts([prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_headline(seed_text, next_words=1):\n",
    "    for _ in range(next_words):\n",
    "        # Predict next token\n",
    "        prediction = predict_next_token(seed_text)\n",
    "        # Convert token to word\n",
    "        next_word = tokenizer.sequences_to_texts([prediction])[0]\n",
    "        # Add next word to the headline. This headline will be used in the next pass of the loop.\n",
    "        seed_text += \" \" + next_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_texts = [\n",
    "    'washington dc is',\n",
    "    'today in new york',\n",
    "    'the school district has',\n",
    "    'crime has become']\n",
    "for seed in seed_texts:\n",
    "    print(generate_headline(seed, next_words=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
